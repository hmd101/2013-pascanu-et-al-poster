
% % LaTeX poster using beamerposter class (3-column layout)
% \documentclass[final]{beamer}

% % Poster theme and packages
% \usepackage[size=a0,scale=1.0]{beamerposter}
% \usepackage{amsmath, amssymb}
% \usepackage{graphicx}
% \usepackage{xcolor}
% \usepackage{multicol}
% \usepackage{tikz}
% \usetikzlibrary{positioning} % <-- added for 'of=' syntax in tikz
% \usepackage[utf8]{inputenc} % Ensure unicode compatibility
% \usepackage{lmodern} % Better font rendering for large fonts

% % Custom NYU Violet color and styling
% \definecolor{nyuviolet}{RGB}{87, 0, 140}
% \setbeamercolor{block title}{bg=nyuviolet, fg=white}
% \setbeamercolor{block body}{bg=white, fg=black}
% \setbeamerfont{block title}{series=\bfseries}

% %% FOR colors used in equations:
% \definecolor{myred}{RGB}{200,0,0}
% \definecolor{myblue}{RGB}{0,0,180}
% % define a big fat red todo command
% \newcommand{\todo}[1]{\textcolor{myred}{\textbf{TODO:} #1}}

% % Title and authors
% \title{On the Difficulty of Training Recurrent Neural Networks}
% \author{Razvan Pascanu, Tomas Mikolov, Yoshua Bengio}
% \institute{ICML 2013}

% % Document
% \begin{document}

% \begin{frame}[t]
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title Block %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \begin{columns}[t,totalwidth=\textwidth]
%     \begin{column}{0.66\textwidth}
%       \vspace{1em}
%       {\centering
%         {\veryHuge\bfseries On the Difficulty of Training RNNs}\\[0.5em]
%         {\Huge R. Pascanu, T. Mikolov, Y. Bengio}\\[0.5em]
%         {\Large\itshape ICML 2013}\\
%       }
%     \end{column}
%     \begin{column}{0.33\textwidth}
%       \flushright
%       \Large \textit{Presented by OUR NAMES: Hanna M. Dettki}
%     \end{column}
%   \end{columns}
%   \vspace{2em}

%   % Main body columns
%   \begin{columns}[t,totalwidth=\textwidth]

%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \begin{column}{0.33\textwidth}
%       \begin{block}{1. Introduction \& Background} \todo{}

%         \textbf{1.1 Context \& Motivation}
%             \begin{itemize}
%             \item \textbf{Importance of sequence modeling}
%             \begin{itemize}
%                 \item e.g., language, time-series in finance
%             \end{itemize}

%             \item Identifying gradient problems (Bengio et al., 1994)
%             \begin{itemize}
%                 \item \textbf{Vanishing gradient problem}: impossible to learn long-term dependencies
%                 \item \textbf{Exploding gradient problem}: numerical instabilities $\rightarrow$ unstable training
%             \end{itemize}

%             \item $\rightarrow$ Why stable gradient flow is critical for learning temporal dependencies (paper's contribution)
%             \end{itemize}

%         \vspace{1em}

%         \textbf{1.2 Schematic \& formal def. of RNN}
%             \begin{tikzpicture}[node distance=2.5cm, thick, >=latex, every node/.style={scale=1.5}, xshift=-1cm]
%             % Nodes
%             \node[draw, minimum height=1.5cm, minimum width=0.8cm] (input) {$u_t$};
%             \node[draw, right=of input, minimum height=1.5cm, minimum width=0.8cm] (rnn) {$x_t$};
%             \node[right=of rnn] (error) {$\mathcal{E}_t$};
            
%             % Arrows
%             \draw[->, line width=1.5pt] (input) -- (rnn);
%             \draw[->, line width=1.5pt] (rnn) -- (error);
            
%             % Recurrent loop (from right to left, meeting top corners)
%             \draw[->, orange, line width=2pt]
%                 ([xshift=3pt,yshift=3pt]rnn.north east)
%                 to[out=45,in=135,looseness=1.2]
%                 ([xshift=-3pt,yshift=3pt]rnn.north west);
            
%             % Label
%             \node[below=0.5cm of input] {Fig. 1};
%             \end{tikzpicture}

%         \vspace{0.5em}
%         \begin{align*}
%           x_t &= F(x_{t-1}, u_t, \theta) && \text{(1) General} \\
%           x_t &= \textcolor{orange}{W_{\text{rec}}} \, \sigma(x_{t-1}) + W_{in} u_t + \mathbf{b} && \text{(2)}
%         \end{align*}

%         \vspace{0.5em}
%         \small
%         where $u_t$: input, $x_t$: state, $t$: time step, $\mathbf{b}$: bias, $\mathcal{E}_t = \mathcal{L}(x_t)$ (error)

%         \vspace{0.5em}
%         \textcolor{orange}{\textbf{The recurrent connections}} in the hidden layer allow information to persist from one input to another.

%         \vspace{1em}
%         \textbf{1.3 Training RNNs: Backprop Through Time (BPTT) on Unrolled RNN}\\
%         \includegraphics[width=0.95\linewidth]{figures/2_fig.png}\\[0.5em]
%         Fig. 2: Unrolled RNN

%         \vspace{0.3em}
%         \begin{align*}
%           \textcolor{myblue}{\frac{\partial \mathcal{E}}{\partial \theta}} &=
%           \sum_{t=1}^{T} \frac{\partial \mathcal{E}_t}{\partial \theta} \hspace{2em} \text{(3)} \\[0.5em]
%           \frac{\partial \mathcal{E}_t}{\partial \theta} &=
%           \sum_{k=1}^{t} 
%           \left( {\frac{\partial \mathcal{E}_t}{\partial x_t}} 
%           \textcolor{myred}{\frac{\partial x_t}{\partial x_k}}
%           \frac{\partial^+ x_k}{\partial \theta} \right) \hspace{2em} \text{(4)} \\[0.5em]
%           \textcolor{myred}{\frac{\partial x_t}{\partial x_k}} &=
%           \prod_{i=k+1}^{t}
%           \textcolor{orange}{W_{\text{rec}}}^\top \cdot \text{diag}\left(\sigma'(x_{i-1})\right) \hspace{2em} \text{(5)}
%         \end{align*}
        
%         \vspace{0.5em}
%         where $\frac{\partial^+ x_k}{\partial \theta}$ denotes the “immediate” partial derivative (treating $x_{k-1}$ as constant).
        
%         \vspace{0.5em}
%         \textcolor{myblue}{Blue}: total gradient over time.
%         \quad
%         \textcolor{myred}{Red}: temporal error contribution.

%       \end{block}
%     \end{column}

%    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{column}{0.33\textwidth}
%     \begin{block}{2. The Problem}

%     \textbf{Mechanics of Exploding and Vanishing Gradients:} \\
%     These issues occur in RNNs due to repeated multiplication of Jacobian matrices during backpropagation. If the spectral radius $\rho$ of the recurrent weight matrix $W_{\text{rec}}$ is less than 1, gradients vanish; if greater than 1, they explode. For non-linear activations with bounded derivatives (e.g., $\gamma=1$ for tanh), gradients vanish when the largest singular value $\lambda_1 < \gamma^{-1}$.

%     \vspace{0.5em}
%     \textbf{Dynamical Systems View:} \\
%     An RNN’s hidden state evolves like a dynamical system converging to attractors. As parameters change, the system may cross bifurcation points, causing drastic changes in state evolution. Crossing basin boundaries can result in gradient explosions. Inputs can shift the system into different attractor basins, intensifying this instability.

%     \begin{center}
%         \includegraphics[width=0.9\linewidth]{figures/bifurcation.png} \\
%         \small Bifurcation diagram showing attractor transitions
%     \end{center}
    
%     \begin{center}
%         \includegraphics[width=0.9\linewidth]{figures/function_and_input.png} \\
%         \small Gradient explosion due to basin crossing
%     \end{center}

%     \vspace{0.5em}
%     \textbf{Geometric Interpretation:} \\
%     Consider $x_t = w\sigma(x_{t-1}) + b$ with $x_0 = 0.5$. In the linear case ($b = 0$), gradients are $\frac{\partial x_t}{\partial w} = t w^{t-1} x_0$, showing exponential growth. Exploding gradients align with steep directions in the error surface, forming sharp walls that SGD struggles to traverse, disrupting convergence.

%     \begin{center}
%         \includegraphics[width=0.9\linewidth]{figures/geometric.png} \\
%         \small Steep error surface caused by exploding gradients
%     \end{center}

%     \end{block}
% \end{column}



%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \begin{column}{0.33\textwidth}
%     \begin{block}{3. Solution \& Experiments}
  
%       \textbf{3.1 Gradient Clipping}
%       \begin{itemize}
%         \item see Fig. 3 for \textit{motivation}
%         \item Pseudo-code for norm-clipping
%         \item \todo{}
%       \end{itemize}
  
%       \vspace{0.5em}
%       \textbf{3.2 VG-Regularization}
%       \begin{itemize}
%         \item see paper eq. 9 \& 10
%         \item  \todo{}
%       \end{itemize}
  
%       \vspace{0.5em}
%       \textbf{3.3 MSGD-CR}
%         \begin{itemize}
%             \item \todo{}
%             \item  (combines 3.1 \& 3.2)
            
%     \end{itemize}
  
%       \vspace{0.5em}
%       \textbf{3.4 Initialization Strategies}
%       \begin{itemize}
%         \item see experiments in Sec. 4
%       \end{itemize}
  
%       \vspace{1em}
%       \begin{center}
%         \fbox{\parbox{0.9\linewidth}{\centering \vspace{10em} \textit{placeholder figure} \vspace{2em}}}
%         \\[0.3em]
%         Fig. 4
%       \end{center}

%       \begin{center}
%         \fbox{\parbox{0.9\linewidth}{\centering \vspace{10em} \textit{placeholder figure} \vspace{2em}}}
%         \\[0.3em]
%         Fig. 5
%       \end{center}
  
%     \end{block}
  
%     \vspace{1em}
  
%     \begin{block}{4. Relevance today \& SOTA techniques} \todo{}
%       \begin{itemize}
%         \item Clipping still relevant!
%         \item Instead of regularization:
%         \begin{itemize}
%           \item residual connections
%           \item gradient checkpointing
%           \item gating mechanisms
%           \item layer normalization
%           \item attention mechanism
%           \item positional encoding
%         \end{itemize}
%       \end{itemize}
%     \end{block}
%   \end{column}
  
%   \end{columns}
% \end{frame}

% \end{document}


% LaTeX poster using beamerposter class (3-column layout)
\documentclass[final]{beamer}
\usepackage{tikz}
\usetikzlibrary{mindmap}

% Define the university color
\definecolor{uniLightViolet1}{HTML}{ab82c5}
\definecolor{uniLightViolet2}{HTML}{eee6f3}

% Poster theme and packages
\usepackage[size=a0,scale=1.0]{beamerposter}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{positioning} % <-- added for 'of=' syntax in tikz
\usepackage[utf8]{inputenc} % Ensure unicode compatibility
\usepackage{lmodern} % Better font rendering for large fonts

% Custom NYU Violet color and styling
\definecolor{nyuviolet}{RGB}{87, 0, 140}
\setbeamercolor{block title}{bg=nyuviolet, fg=white}
\setbeamercolor{block body}{bg=white, fg=black}
\setbeamerfont{block title}{series=\bfseries}

%% FOR colors used in equations:
\definecolor{myred}{RGB}{200,0,0}
\definecolor{myblue}{RGB}{0,0,180}
% define a big fat red todo command
\newcommand{\todo}[1]{\textcolor{myred}{\textbf{TODO:} #1}}

% Title and authors
\title{On the Difficulty of Training Recurrent Neural Networks}
\author{Razvan Pascanu, Tomas Mikolov, Yoshua Bengio}
\institute{ICML 2013}

% Document
\begin{document}

\begin{frame}[t]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title Block %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{columns}[t,totalwidth=\textwidth]
    \begin{column}{0.66\textwidth}
      \vspace{1em}
      {\centering
        {\veryHuge\bfseries On the Difficulty of Training RNNs}\\[0.5em]
        {\Huge R. Pascanu, T. Mikolov, Y. Bengio}\\[0.5em]
        {\Large\itshape ICML 2013}\\
      }
    \end{column}
    \begin{column}{0.33\textwidth}
      \flushright
      \Large \textit{Presented by OUR NAMES: Hanna M. Dettki}
    \end{column}
  \end{columns}
  \vspace{2em}

  % Main body columns
  \begin{columns}[t,totalwidth=\textwidth]

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{column}{0.33\textwidth}
      \begin{block}{1. Introduction \& Background} \todo{}

        \textbf{1.1 Context \& Motivation}
            \begin{itemize}
            \item \textbf{Importance of sequence modeling}
            \begin{itemize}
                \item e.g., language, time-series in finance
            \end{itemize}

            \item Identifying gradient problems (Bengio et al., 1994)
            \begin{itemize}
                \item \textbf{Vanishing gradient problem}: impossible to learn long-term dependencies
                \item \textbf{Exploding gradient problem}: numerical instabilities $\rightarrow$ unstable training
            \end{itemize}

            \item $\rightarrow$ Why stable gradient flow is critical for learning temporal dependencies (paper's contribution)
            \end{itemize}

        \vspace{1em}

        \textbf{1.2 Schematic \& formal def. of RNN}
            \begin{tikzpicture}[node distance=2.5cm, thick, >=latex, every node/.style={scale=1.5}, xshift=-1cm]
            % Nodes
            \node[draw, minimum height=1.5cm, minimum width=0.8cm] (input) {$u_t$};
            \node[draw, right=of input, minimum height=1.5cm, minimum width=0.8cm] (rnn) {$x_t$};
            \node[right=of rnn] (error) {$\mathcal{E}_t$};
            
            % Arrows
            \draw[->, line width=1.5pt] (input) -- (rnn);
            \draw[->, line width=1.5pt] (rnn) -- (error);
            
            % Recurrent loop (from right to left, meeting top corners)
            \draw[->, orange, line width=2pt]
                ([xshift=3pt,yshift=3pt]rnn.north east)
                to[out=45,in=135,looseness=1.2]
                ([xshift=-3pt,yshift=3pt]rnn.north west);
            
            % Label
            \node[below=0.5cm of input] {Fig. 1};
            \end{tikzpicture}

        \vspace{0.5em}
        \begin{align*}
          x_t &= F(x_{t-1}, u_t, \theta) && \text{(1) General} \\
          x_t &= \textcolor{orange}{W_{\text{rec}}} \, \sigma(x_{t-1}) + W_{in} u_t + \mathbf{b} && \text{(2)}
        \end{align*}

        \vspace{0.5em}
        \small
        where $u_t$: input, $x_t$: state, $t$: time step, $\mathbf{b}$: bias, $\mathcal{E}_t = \mathcal{L}(x_t)$ (error)

        \vspace{0.5em}
        \textcolor{orange}{\textbf{The recurrent connections}} in the hidden layer allow information to persist from one input to another.

        \vspace{1em}
        \textbf{1.3 Training RNNs: Backprop Through Time (BPTT) on Unrolled RNN}\\
        \includegraphics[width=0.95\linewidth]{figures/2_fig.png}\\[0.5em]
        Fig. 2: Unrolled RNN

        \vspace{0.3em}
        \begin{align*}
          \textcolor{myblue}{\frac{\partial \mathcal{E}}{\partial \theta}} &=
          \sum_{t=1}^{T} \frac{\partial \mathcal{E}_t}{\partial \theta} \hspace{2em} \text{(3)} \\[0.5em]
          \frac{\partial \mathcal{E}_t}{\partial \theta} &=
          \sum_{k=1}^{t} 
          \left( {\frac{\partial \mathcal{E}_t}{\partial x_t}} 
          \textcolor{myred}{\frac{\partial x_t}{\partial x_k}}
          \frac{\partial^+ x_k}{\partial \theta} \right) \hspace{2em} \text{(4)} \\[0.5em]
          \textcolor{myred}{\frac{\partial x_t}{\partial x_k}} &=
          \prod_{i=k+1}^{t}
          \textcolor{orange}{W_{\text{rec}}}^\top \cdot \text{diag}\left(\sigma'(x_{i-1})\right) \hspace{2em} \text{(5)}
        \end{align*}
        
        \vspace{0.5em}
        where $\frac{\partial^+ x_k}{\partial \theta}$ denotes the “immediate” partial derivative (treating $x_{k-1}$ as constant).
        
        \vspace{0.5em}
        \textcolor{myblue}{Blue}: total gradient over time.
        \quad
        \textcolor{myred}{Red}: temporal error contribution.

      \end{block}
    \end{column}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{0.33\textwidth}
    \begin{block}{2. The Problem}

    \textbf{Mechanics of Exploding and Vanishing Gradients:} \\
    These issues occur in RNNs due to repeated multiplication of Jacobian matrices during backpropagation. If the spectral radius $\rho$ of the recurrent weight matrix $W_{\text{rec}}$ is less than 1, gradients vanish; if greater than 1, they explode. For non-linear activations with bounded derivatives (e.g., $\gamma=1$ for tanh), gradients vanish when the largest singular value $\lambda_1 < \gamma^{-1}$.

    \vspace{0.5em}
    \textbf{Dynamical Systems View:} \\
    An RNN’s hidden state evolves like a dynamical system converging to attractors. As parameters change, the system may cross bifurcation points, causing drastic changes in state evolution. Crossing basin boundaries can result in gradient explosions. Inputs can shift the system into different attractor basins, intensifying this instability.

    \begin{center}
        \includegraphics[width=0.9\linewidth]{figures/bifurcation.png} \\
        \small Bifurcation diagram showing attractor transitions
    \end{center}
    
    \begin{center}
        \includegraphics[width=0.9\linewidth]{figures/function_and_input.png} \\
        \small Gradient explosion due to basin crossing
    \end{center}

    \vspace{0.5em}
    \textbf{Geometric Interpretation:} \\
    Consider $x_t = w\sigma(x_{t-1}) + b$ with $x_0 = 0.5$. In the linear case ($b = 0$), gradients are $\frac{\partial x_t}{\partial w} = t w^{t-1} x_0$, showing exponential growth. Exploding gradients align with steep directions in the error surface, forming sharp walls that SGD struggles to traverse, disrupting convergence.

    \begin{center}
        \includegraphics[width=0.9\linewidth]{figures/geometric.png} \\
        \small Steep error surface caused by exploding gradients
    \end{center}

    \end{block}
\end{column}



    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Column 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{column}{0.33\textwidth}
    \begin{block}{3. Solution \& Experiments}
        \textbf{Gradient Clipping}
        
        \vspace{-2em}
    
        \begin{minipage}{0.54\textwidth}
        \begin{itemize}
          \item Pseudo-code: $\hat{g} \leftarrow \nabla E;\ \text{if } \|\hat{g}\|_2 \geq \tau \text{ then } \hat{g} \leftarrow \tau \cdot \frac{\hat{g}}{\|\hat{g}\|_2}$
          \item Gradient clipping introduces a hyperparameter: the threshold. A common heuristic sets this value based on the average gradient norm over early training steps.
          \item Compared to clipping individual gradient components by value, norm-based clipping preserves the direction of the gradient vector and is generally more robust in high-dimensional settings.
        \end{itemize}
        \end{minipage}
        \hfill
        \begin{minipage}{0.42\textwidth}
        \centering
        \vspace{1em} % pulls the image up
        \includegraphics[width=\linewidth]{figures/gradient_clipping.png}
        % \vspace{-0.8em} % reduce space below
        \\[-0.3em]
        \end{minipage}

      \textbf{Vanishing Gradient Regularization}
      \begin{itemize}
        \item Regularizer: \[
            \Omega = \sum_k \Omega_k = \sum_k \left(  \frac{ \left\| \frac{\partial E}{\partial x_{k+1}} \cdot \frac{\partial x_{k+1}}{\partial x_k} \right\| }
            { \left\| \frac{\partial E}{\partial x_{k+1}} \right\| } - 1 
            \right)^2
            \]
            \[
            \textstyle
            \frac{\partial^+ \Omega}{\partial W_{\text{rec}}} = \sum_k 
            \frac{\partial^+}{\partial W_{\text{rec}}} \left(
            \left( 
            \frac{ \left\| \frac{\partial E}{\partial x_{k+1}} \cdot W_{\text{rec}}^\top \cdot \operatorname{diag}\left( \sigma'(x_k) \right) \right\|^2 }
            { \left\| \frac{\partial E}{\partial x_{k+1}} \right\|^2 } - 1 
            \right)^2
            \right)
            \]

        \item The regularization term only enforces norm preservation of the Jacobian matrix
                $\frac{\partial x_{k+1}}{\partial x_k}$ in the direction of the error signal 
                $\frac{\partial E}{\partial x_{k+1}}$, not in all directions.
        \item The soft constraint does not guarantee perfect norm preservation, so exploding gradients may still occur, particularly during early training or unstable updates. To mitigate this, we combine the regularizer with gradient clipping for more stable and effective learning.


      \end{itemize}

  \textbf{Experiments and Results}
%   \begin{tikzpicture} 
% [mindmap,
%   grow cyclic,
%   every node/.style={concept,minimum size=4.2cm,inner sep=0pt,},
%   concept color=uniLightViolet2,
%   text width=7cm,
%   align=left,
%   level 1/.append style={level distance=10cm,font=\normalsize, minimum size=4.5cm, align=left,},
%   level 2/.append style={level distance=6cm, sibling angle=45, font=\small, minimum size=4cm, align=left,},
% ]
% \node [font=\normalsize, minimum size=5cm, align=flush center]{Two types\\of\\Problems}
%   child [grow=180,concept color=uniLightViolet2, align = left, minimum size=5cm] { node {{Pathological\\Problems}}
%     child [grow=60]{ node {Temporal\\Order\\ Problem}}
%     child [grow=100]{ node {Addition\\Problem}}
%     child [grow=145]{ node {Multiplication\\Problem}}
%     child [grow=190]{ node {3-bit\\Temporal\\ Problem}}
%     child [grow=245]{ node {Random\\Permutation\\Problem}}
%     child [grow=325] { node {Noiseless\\ Memorization\\ Problem}}
%   }
%   child [grow=0, concept color=uniLightViolet2,  minimum size=7cm, align=left] { node {Natural\\ Problems}
%     child [grow=60]{ node {Polyphonic\\Music\\Prediction}}
%     child [grow=135]{ node {Language\\Modeling\\(character level)}}
%   }
% ;
% \end{tikzpicture}
% Add vertical space after the mindmap
  \vspace{0.5cm}

  % Add the heading and 6-7 lines of text
  \textbf{Experiments and Results To edit} \\
  This section presents the experimental setup and results. \\
  evaluated the performance of different initialization methods. \\
  The temporal order problem was tested across various sequence lengths. \\
  Three initialization techniques were compared: sigmoid, basic tanh, and smart tanh. \\
  The results highlight the impact of initialization on success rates. \\
  Detailed analysis shows smart tanh outperforming the others. \\
  See the figure below for a visual comparison of the results.

  % Add vertical space before the image
  \vspace{0.5cm}
\begin{center}
  \includegraphics[width=0.9\textwidth]{figures/results.png}
  %\vspace{0.2cm}
  \captionof{\\Figure 7: Rate of success for solving the temporal order problem versus sequence length for different initializations (from left to right: sigmoid, basic tanh and smart tanh)}
\end{center}
 \vspace{0.5cm}

  % Add the heading and 6-7 lines of text
  \textbf{Results - what worked } \\
  Line 1 \\
  Line 2. \\
  Line 3. \\
  Line 4 \\
  Line 5 \\
  Line 6\\
  Line 7\\
  Line 8 \\
  Line 9\\
  Line 10
% if using 3 images separately
% \begin{figure}[h]
%   \centering
%   \begin{minipage}{0.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/sigmoid.png}
%     \caption{sigmoid.png}
%   \end{minipage}\hfill
%   \begin{minipage}{0.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/basic_tanh.png}
%     \caption{basic_tanh.png}
%   \end{minipage}\hfill
%   \begin{minipage}{0.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/smart_tanh.png}
%     \caption{smart_tanh.png}
%   \end{minipage}
% \end{figure}

    \end{block}

    \vspace{1em}
  
    \begin{block}{4. Relevance today \& SOTA techniques}
        %%% SUMMARY
        % \begin{table}[h!]
        % \centering
        % \begin{tabular}{|c|c|c|}
        % \hline
        % \textbf{Technique} & \textbf{Advantages} & \textbf{Disadvantages} \\ \hline
        % Gradient Clipping & Prevents exploding gradients & Requires tuning of threshold \\ \hline
        % Regularization & Mitigates vanishing gradients & May not fully prevent instability \\ \hline
        % Initialization Strategies & Improves convergence & Sensitive to parameter choice \\ \hline
        % \end{tabular}
        % \caption{Comparison of Techniques for Addressing Gradient Issues in RNNs}
        % \end{table}

 
            %%% reorganized bullet points below in more space-efficient way
        
            \begin{tabular}{@{}ll@{}}
                \textbf{Exploding gradients:} & \textbf{Clipping is still relevant!} \\
                \textbf{Vanishing gradients:} & 
                    \begin{minipage}[t]{0.8\textwidth} 
                        \textbf{Alternatives to regularization:}\\[0.3em]
                        \begin{tabular}{@{}lll@{}}
                        $\bullet$ Residual connections     & $\bullet$ Gating mechanisms       & $\bullet$ Attention mechanism \\
                        $\bullet$ Gradient checkpointing   & $\bullet$ Layer normalization     & $\bullet$ Positional encoding \\
                        \end{tabular}
                    \end{minipage} \\
                \end{tabular}
                
    
                

    %   \begin{itemize}
    %     \item Clipping still relevant!
    %     \item Instead of regularization:
    %     \begin{itemize}
    %       \item residual connections
    %       \item gradient checkpointing
    %       \item gating mechanisms
    %       \item layer normalization
    %       \item attention mechanism
    %       \item positional encoding
    %     \end{itemize}
    %   \end{itemize}
    \end{block}
  \end{column}
  
  \end{columns}
\end{frame}

\end{document}
